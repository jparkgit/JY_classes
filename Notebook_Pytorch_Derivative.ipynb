{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jparkgit/NumPy/blob/master/Notebook_Pytorch_Derivative.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "10b153d3-dc8d-4202-a397-75b32ac07943"
      },
      "source": [
        "# Chapter 2. Pytorch\n",
        "\n",
        "## Derivative\n"
      ],
      "id": "10b153d3-dc8d-4202-a397-75b32ac07943"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dpnr7dNDxiMz"
      },
      "source": [
        "# 3. Derivatives"
      ],
      "id": "dpnr7dNDxiMz"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7FN2-gozxiMz"
      },
      "source": [
        "## 3.1. Derivatives"
      ],
      "id": "7FN2-gozxiMz"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W8tXij_yxiMz"
      },
      "source": [
        "Determine the derivative of\n",
        "\n",
        "$ y = 2x^3+x $   \n",
        "\n",
        "at   $x=1$.\n"
      ],
      "id": "W8tXij_yxiMz"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VWRHXs90xiMz",
        "outputId": "d229f545-3c80-42ed-f43c-11e3a7afbaa4"
      },
      "source": [
        "x = torch.tensor(1., requires_grad=True) #every operation on them should be tracked\n",
        "\n",
        "y = 2 * x ** 3 + x\n",
        "\n",
        "y.backward() # calculate derivatives and stores them in the respective tensors' .grad\n",
        "\n",
        "x.grad"
      ],
      "id": "VWRHXs90xiMz",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(7.)"
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gV739GmyxiM0"
      },
      "source": [
        "$ \\frac{dy(x)}{dx} = 6x^{2} + 1$  "
      ],
      "id": "gV739GmyxiM0"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EM9vTVcfxiM0"
      },
      "source": [
        "$ \\frac{dy(x=1)}{dx} = 6*1^{2} + 1$"
      ],
      "id": "EM9vTVcfxiM0"
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.tensor(1., requires_grad=True) #every operation on them should be tracked\n",
        "y = 2 * x ** 3 + x\n",
        "z = torch.exp(y)\n",
        "z.backward()\n",
        "x.grad"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TnaMgoqAIxqt",
        "outputId": "782f81be-2097-45c5-8751-11d91f082963"
      },
      "id": "TnaMgoqAIxqt",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(140.5988)"
            ]
          },
          "metadata": {},
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mIuxPdl9xiM0",
        "outputId": "b4017f19-b5f1-4580-93c7-75a542853ac6"
      },
      "source": [
        "print(x)\n",
        "print('data:',x.data)\n",
        "print('grad:',x.grad)\n",
        "\n"
      ],
      "id": "mIuxPdl9xiM0",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(1., requires_grad=True)\n",
            "data: tensor(1.)\n",
            "grad: tensor(140.5988)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pwe9nvR-zJQU",
        "outputId": "83f08f23-6093-429b-db93-dfc8b8d33810"
      },
      "source": [
        "print(\"is_leaf:\",x.is_leaf)\n",
        "print(\"requires_grad:\",x.requires_grad)"
      ],
      "id": "pwe9nvR-zJQU",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "is_leaf: True\n",
            "requires_grad: True\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zVK8Xu_xxiM0",
        "outputId": "e2336a62-b88a-4ba8-a22a-683ab9f0bc9f"
      },
      "source": [
        "print('data:',y.data)\n",
        "print('grad_fn:',y.grad_fn)\n",
        "print('grad:',y.grad)\n"
      ],
      "id": "zVK8Xu_xxiM0",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "data: tensor(3.)\n",
            "grad_fn: <AddBackward0 object at 0x7ff520953dd0>\n",
            "grad: None\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:3: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more information.\n",
            "  This is separate from the ipykernel package so we can avoid doing imports until\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e0Os9lrazTu8",
        "outputId": "31b41573-c46c-445a-fc20-e78ee15fd043"
      },
      "source": [
        "print(\"is_leaf:\",y.is_leaf)\n",
        "print(\"requires_grad:\",y.requires_grad)"
      ],
      "id": "e0Os9lrazTu8",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "is_leaf: False\n",
            "requires_grad: True\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4hMH1w1FxiM1"
      },
      "source": [
        "### 2-4. Partial Derivates"
      ],
      "id": "4hMH1w1FxiM1"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9QcAgKyuxiM1"
      },
      "source": [
        "#### 참고  \n",
        "* `requires_grads=True`인 tensor는 tensor를 바로 .numpy() 로 변환 불가능  \n",
        "* `.detach().numpy()` 로 변환 가능"
      ],
      "id": "9QcAgKyuxiM1"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d7U-W4ojxiM2"
      },
      "source": [
        "Try to determine partial derivative  $u$ of the following function where $u=2$ and $v=1$: $ f=uv+(uv)^2$\n"
      ],
      "id": "d7U-W4ojxiM2"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DwVCRvBIxiM2",
        "outputId": "299cbd75-f74d-4e09-a664-06b08d51dd19"
      },
      "source": [
        "# Practice: Calculate the derivative of f = u * v + (u * v) ** 2 at u = 2, v = 1\n",
        "\n",
        "# Type the code here\n",
        "u = torch.tensor(2., requires_grad=True)\n",
        "v = torch.tensor(1., requires_grad=True)\n",
        "y = u*v + (u*v)**2\n",
        "\n",
        "y.backward()\n",
        "print(u.grad, v.grad)"
      ],
      "id": "DwVCRvBIxiM2",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor(5.) tensor(10.)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q2uzQh170r1_"
      },
      "source": [
        "### Exercise 2:\n",
        "You are given\n",
        "\n",
        "$y=\\exp(2x_1)+x_2^2$.\n",
        "\n",
        "Calculate $\\triangledown y = \\left( \\frac{\\delta y}{\\delta x_1},\n",
        "\\frac{\\delta y}{\\delta x_2} \\right)$ at $x_1=1.0$ and $x_2=2.0$."
      ],
      "id": "q2uzQh170r1_"
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.tensor([1.0, 2.0], requires_grad=True, dtype=torch.float32)\n",
        "y=torch.exp(2*x[0])+x[1]**2\n",
        "y.backward()\n",
        "x.grad"
      ],
      "metadata": {
        "id": "ioSxcy8ukLOr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8ee517a4-2921-4252-8c93-b65a10b3afbb"
      },
      "id": "ioSxcy8ukLOr",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([14.7781,  4.0000])"
            ]
          },
          "metadata": {},
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### See why the following is not working\n",
        "x = torch.tensor([1.0, 2.0], requires_grad=True, dtype=torch.float32)\n",
        "y=torch.math.exp(2*x[0])\n",
        "y.backward()\n",
        "x.grad"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 228
        },
        "id": "kSjDvNJC8hxm",
        "outputId": "949545b5-1b3d-4916-ca4e-1aa4fa337021"
      },
      "id": "kSjDvNJC8hxm",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-62-ac2574918c29>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2.0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequires_grad\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'float' object has no attribute 'backward'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.5 Chain rules amd gradients\n",
        "\n"
      ],
      "metadata": {
        "id": "ZC49BTTdvuc7"
      },
      "id": "ZC49BTTdvuc7"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Example 1:\n",
        "\n",
        "You are given $f(x_1, x_2)=(x_1+2x_2^3)^2$.\n",
        "\n",
        "* Calculate grad $f$ at $(x_1, x_2)=(1,2)$.\n",
        "\n",
        "Here, we are presenting two solutions. What is the difference?"
      ],
      "metadata": {
        "id": "VkS3YLbk-i4u"
      },
      "id": "VkS3YLbk-i4u"
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.tensor([1.0, 2.0], requires_grad=True)\n",
        "f = (x[0]+2*x[1]**3)**2\n",
        "f.backward()\n",
        "x.grad"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i85LbKcH_Mbb",
        "outputId": "284b9da8-2955-4d0c-d815-0cccd63322bb"
      },
      "id": "i85LbKcH_Mbb",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([ 34., 816.])"
            ]
          },
          "metadata": {},
          "execution_count": 73
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.tensor([[1.0, 2.0]], requires_grad=True)\n",
        "f = (x[0,0]+2*x[0,1]**3)**2\n",
        "f.backward()\n",
        "x.grad"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q0mwBaC_-iND",
        "outputId": "5e67dbb7-874c-4074-9a83-25eeec6120b9"
      },
      "id": "Q0mwBaC_-iND",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 34., 816.]])"
            ]
          },
          "metadata": {},
          "execution_count": 74
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Example 2:\n",
        "\n",
        "Let $f(x)=x_1^2+2x_2$ and $g(t)=\\begin{pmatrix} \\sin(t) \\\\ \\cos(t)\\end{pmatrix}.$"
      ],
      "metadata": {
        "id": "itswXHsE-gGG"
      },
      "id": "itswXHsE-gGG"
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "t=torch.tensor([1.0], requires_grad=True)\n",
        "g=torch.sin(t)**2+2* torch.cos(t)\n",
        "g.backward()\n",
        "t.grad"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xwZYKhzk8oGO",
        "outputId": "8931b818-2401-428d-baf3-4184854698a7"
      },
      "id": "xwZYKhzk8oGO",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([-0.7736])"
            ]
          },
          "metadata": {},
          "execution_count": 66
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "t=torch.tensor([1.0], requires_grad=True)\n",
        "g=[torch.sin(t), torch.cos(t)]\n",
        "f=g[0]**2 + 2*g[1]\n",
        "f.backward()\n",
        "print(t.grad)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "We0PnA-UvtTc",
        "outputId": "f2d42fc2-9db7-4320-cbb7-d1087dc9d729"
      },
      "id": "We0PnA-UvtTc",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([-0.7736])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "piaO4B6lwQsU"
      },
      "id": "piaO4B6lwQsU",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.6$^*$ Jacobian (Second derivatives)\n",
        "\n"
      ],
      "metadata": {
        "id": "Qb2YElefSeWn"
      },
      "id": "Qb2YElefSeWn"
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "def myftn(x, y):\n",
        "    return 2 * torch.exp(x) + 3 * y\n",
        "\n",
        "inputs = (torch.rand(1), torch.rand(1)) # arguments for the function\n",
        "print(inputs)\n",
        "torch.autograd.functional.jacobian(myftn, inputs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hNmCSKBbSink",
        "outputId": "77149355-6d87-41cb-dfea-5c38d04683a2"
      },
      "id": "hNmCSKBbSink",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(tensor([0.3463]), tensor([0.5210]))\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([[2.8276]]), tensor([[3.]]))"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## HW 1:\n",
        "\n",
        "You are given $X$ and $Y$. Using the linear regression, you want to have the following model\n",
        "\n",
        "$\\widehat{Y}=\\widehat{\\beta}_0+\\widehat{\\beta}_1X$\n",
        "\n",
        "by minimizing the following square loss\n",
        "\n",
        "$g(\\beta_0, \\beta_1)=\\sum\\limits_{i=1}^{20}(y_i-(\\beta_0+\\beta_1 x_i))^2$.\n",
        "\n",
        "Answer the following questions.\n",
        "\n",
        "1. Calculate grad $g$ at $(\\beta_0, \\beta_1)=(1.0, 0.5)$ without using autograd.\n",
        "\n",
        "2. Calculate grad $g$ at $(\\beta_0, \\beta_1)=(1.0, 0.5)$ using autograd.\n",
        "\n",
        "3. Reshape $X$ into the shape of $[20,1]$ and name it as $XX$. Then, calculate grad $g$ at $(\\beta_0, \\beta_1)=(1.0, 0.5)$ using autograd. (Caution: your answer should be the same as in 2.)\n",
        "\n",
        "4. Stack $X$ and $Y$ to make $MY\\_data1$ of shape $[20,2]$. Then, calculate grad $g$ at $(\\beta_0, \\beta_1)=(1.0, 0.5)$ using autograd. (Caution: your answer should be the same as in 2.)\n",
        "\n",
        "5. Concatenate $X$ and $Y$ to make $MY\\_data2$ of shape $[20,2]$. (Use the reshape if necessary) Then, calculate grad $g$ at $(\\beta_0, \\beta_1)=(1.0, 0.5)$ using autograd. (Caution: your answer should be the same as in 2.)"
      ],
      "metadata": {
        "id": "c4RGZgC-BPcx"
      },
      "id": "c4RGZgC-BPcx"
    },
    {
      "cell_type": "code",
      "source": [
        "X=torch.tensor([-3.0000e+00, -2.7000e+00, -2.4000e+00, -2.1000e+00, -1.8000e+00,\n",
        "        -1.5000e+00, -1.2000e+00, -9.0000e-01, -6.0000e-01, -3.0000e-01,\n",
        "        -2.3842e-08,  3.0000e-01,  6.0000e-01,  9.0000e-01,  1.2000e+00,\n",
        "         1.5000e+00,  1.8000e+00,  2.1000e+00,  2.4000e+00,  2.7000e+00])"
      ],
      "metadata": {
        "id": "rj-hsS6h7rl1"
      },
      "id": "rj-hsS6h7rl1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Y = torch.tensor([-7.1452, -5.4253, -5.1977, -3.6225, -3.8022, -4.4101, -4.6622, -3.1932,\n",
        "        -1.7325, -1.8879, -1.0742, -0.2320,  1.8226,  1.5453, -1.5535,  0.8857,\n",
        "         1.7537,  3.1607,  1.8912,  4.0895])"
      ],
      "metadata": {
        "id": "-J--aAeqBTKV"
      },
      "id": "-J--aAeqBTKV",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "HVUuut1iBXLM"
      },
      "id": "HVUuut1iBXLM",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## HW 2:\n",
        "\n",
        "You are given\n",
        "\n",
        "$f(x)=x^3-3x^2+4$\n",
        "\n",
        "Answer the following questions.\n",
        "\n",
        "1. Draw the graph of $(x,f(x))$ for $x\\in [-5, 5]$.\n",
        "2. Draw the graph of $(x,f^{\\prime}(x))$ for $x\\in [-5, 5]$ using augograd in torch.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "slWy4oJhDtD-"
      },
      "id": "slWy4oJhDtD-"
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ea3ywnhlEH1Q"
      },
      "id": "ea3ywnhlEH1Q",
      "execution_count": null,
      "outputs": []
    }
  ]
}